{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "con1 = jieba.cut(\"腾讯QQ黄钻三个月QQ黄钻3个月季卡官方自动充值可查时间可续费\")\n",
    "con2  = jieba.cut(\"腾讯q币充值30元qq币30qb30q币自动充值\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\LOUIS_~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 3.312 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['腾讯',\n",
       " 'QQ',\n",
       " '黄钻',\n",
       " '三个',\n",
       " '月',\n",
       " 'QQ',\n",
       " '黄钻',\n",
       " '3',\n",
       " '个',\n",
       " '月季',\n",
       " '卡',\n",
       " '官方',\n",
       " '自动',\n",
       " '充值',\n",
       " '可',\n",
       " '查',\n",
       " '时间',\n",
       " '可',\n",
       " '续费']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换成list\n",
    "content1 = list(con1)\n",
    "content2 = list(con2)\n",
    "content1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯 QQ 黄钻 三个 月 QQ 黄钻 3 个 月季 卡 官方 自动 充值 可 查 时间 可 续费'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把列表转换成字符串\n",
    "c1 = ' '.join(content1)\n",
    "c2 = ' '.join(content2)\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'腾讯 q 币 充值 30 元 qq 币 30qb30q 币 自动 充值'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "data = cv.fit_transform([c1,c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['30', '30qb30q', 'qq', '三个', '充值', '官方', '时间', '月季', '续费', '腾讯', '自动', '黄钻']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 2 1 1 1 1 1 1 1 1 2]\n",
      " [1 1 1 0 2 0 0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 11)\t2\n",
      "  (0, 2)\t2\n",
      "  (0, 9)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 10)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 9)\t1\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "data = tf.fit_transform([c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['30', '30qb30q', 'qq', '三个', '充值', '官方', '时间', '月季', '续费', '腾讯', '自动', '黄钻']\n"
     ]
    }
   ],
   "source": [
    "print(tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.401788   0.28234951 0.200894   0.28234951\n",
      "  0.28234951 0.28234951 0.28234951 0.200894   0.200894   0.56469902]\n",
      " [0.42471719 0.42471719 0.30218978 0.         0.60437955 0.\n",
      "  0.         0.         0.         0.30218978 0.30218978 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t0.20089400062709675\n",
      "  (0, 2)\t0.4017880012541935\n",
      "  (0, 11)\t0.5646990166192883\n",
      "  (0, 3)\t0.28234950830964417\n",
      "  (0, 7)\t0.28234950830964417\n",
      "  (0, 5)\t0.28234950830964417\n",
      "  (0, 10)\t0.20089400062709675\n",
      "  (0, 4)\t0.20089400062709675\n",
      "  (0, 6)\t0.28234950830964417\n",
      "  (0, 8)\t0.28234950830964417\n",
      "  (1, 9)\t0.30218977576862155\n",
      "  (1, 2)\t0.30218977576862155\n",
      "  (1, 10)\t0.30218977576862155\n",
      "  (1, 4)\t0.6043795515372431\n",
      "  (1, 0)\t0.42471718586982765\n",
      "  (1, 1)\t0.42471718586982765\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x12 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 来自CSDN的jieba案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 官方例题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他, 来到, 了, 网易, 杭研, 大厦\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "茂源, 皮压, 牛皮, 骨头, 2, 寸, 10, 支,  , 狗狗,  , 磨牙, 骨,  , 咬胶,  , 洁齿, 棒, 宠物, 宠物狗,  , 零食\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search(\"茂源皮压牛皮骨头2寸10支 狗狗 磨牙骨 咬胶 洁齿棒宠物狗 零食\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "茂源, 皮压, 牛皮, 骨头, 2, 寸, 10, 支,  , 狗狗,  , 磨牙, 骨,  , 咬胶,  , 洁齿, 棒, 宠物狗,  , 零食\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"茂源皮压牛皮骨头2寸10支 狗狗 磨牙骨 咬胶 洁齿棒宠物狗 零食\")\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于TF-IDF算法的关键词抽取    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "· sentence ：为待提取的文本\n",
    "· topK： 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "· withWeight ： 为是否一并返回关键词权重值，默认值为 False\n",
    "· allowPOS ： 仅包括指定词性的词，默认值为空，即不筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "sentence = '全国港澳研究会会长徐泽在会上发言指出，学习系列重要讲话要深刻领会 主席关于香港回归后的宪制基础和宪制秩序的论述'\n",
    "keywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=True, allowPOS=('n', 'nr', 'ns'))\n",
    "print(type(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宪制 2.64150609428\n",
      "香港回归 1.25143832909\n",
      "徐泽 1.19547675029\n",
      "研究会 0.8391289315350001\n",
      "会长 0.778921031247\n",
      "秩序 0.733400522217\n",
      "主席 0.577890023281\n",
      "基础 0.476323060552\n",
      "全国 0.47453215883300004\n"
     ]
    }
   ],
   "source": [
    "for item in keywords:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于 TextRank 算法的关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "·jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=(‘ns’, ‘n’, ‘vn’, ‘v’)) 直接使用，接口相同，注意默认过滤词性。\n",
    "·jieba.analyse.TextRank() 新建自定义 TextRank 实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = jieba.analyse.textrank(sentence, topK=20, withWeight=True, allowPOS=('n', 'nr', 'ns'))\n",
    "type(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "宪制 1.0\n",
      "基础 0.6014196504589024\n",
      "研究会 0.5744061050482787\n",
      "徐泽 0.5726396708599704\n",
      "全国 0.5707257551029473\n",
      "会长 0.5686520443734533\n",
      "香港回归 0.5385671555512545\n",
      "秩序 0.4501072558565364\n",
      "主席 0.22545564673089924\n"
     ]
    }
   ],
   "source": [
    "for item in keywords:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "# 官方例程\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "words = pseg.cut(\"我爱北京天安门\")\n",
    "# words类别为：generator\n",
    "\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "sentence = \"At eight o'clock on Thursday morning\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "la = np.linalg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
