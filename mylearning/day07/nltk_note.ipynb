{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 英文句子的拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"At eight o'clock on Thursday morning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不能拆分中文句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"当然，并不算完美，可是对于基本的英文中的时间表述，抽取和normalization做的都算不错。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['当然，并不算完美，可是对于基本的英文中的时间表述，抽取和normalization做的都算不错。']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba分词拆分中文句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\LOUIS_~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.100 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 我/ 来到/ 北京/ 清华大学\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他, 来到, 了, 网易, 杭研, 大厦\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### github上的jieba 案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "from __future__ import print_function, unicode_literals\n",
    "import jieba.posseg as pseg\n",
    "import jieba\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载自定义字典\n",
    "jieba.load_userdict(\"./dict/user_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.add_word('石墨烯')\n",
    "jieba.add_word('凱特琳')\n",
    "jieba.del_word('自定义词')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = (\n",
    "    \"李小福是创新办主任也是云计算方面的专家; 什么是八一双鹿\\n\"\n",
    "    \"例如我输入一个带“韩玉赏鉴”的标题，在自定义词库中也增加了此词为N类\\n\"\n",
    "    \"「台中」正確應該不會被切開。mac上可分出「石墨烯」；此時又可以分出來凱特琳了。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李小福/是/创新办/主任/也/是/云计算/方面/的/专家/;/ /什么/是/八一双鹿/\n",
      "/例如/我/输入/一个/带/“/韩玉赏鉴/”/的/标题/，/在/自定义/词库/中/也/增加/了/此/词为/N/类/\n",
      "/「/台中/」/正確/應該/不會/被/切開/。/mac/上/可/分出/「/石墨烯/」/；/此時/又/可以/分出/來/凱特琳/了/。\n"
     ]
    }
   ],
   "source": [
    "words = jieba.cut(test_sent)\n",
    "print('/'.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pseg.cut(test_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李小福 / nr ,  是 / v ,  创新办 / i ,  主任 / b ,  也 / d ,  是 / v ,  云计算 / x ,  方面 / n ,  的 / uj ,  专家 / n ,  ; / x ,    / x ,  什么 / r ,  是 / v ,  八一双鹿 / nz ,  \n",
      " / x ,  例如 / v ,  我 / r ,  输入 / v ,  一个 / m ,  带 / v ,  “ / x ,  韩玉赏鉴 / nz ,  ” / x ,  的 / uj ,  标题 / n ,  ， / x ,  在 / p ,  自定义 / l ,  词库 / n ,  中 / f ,  也 / d ,  增加 / v ,  了 / ul ,  此 / r ,  词 / n ,  为 / p ,  N / eng ,  类 / q ,  \n",
      " / x ,  「 / x ,  台中 / s ,  」 / x ,  正確 / ad ,  應該 / v ,  不 / d ,  會 / v ,  被 / p ,  切開 / ad ,  。 / x ,  mac / eng ,  上 / f ,  可 / v ,  分出 / v ,  「 / x ,  石墨烯 / x ,  」 / x ,  ； / x ,  此時 / c ,  又 / d ,  可以 / c ,  分出 / v ,  來 / zg ,  凱特琳 / nz ,  了 / ul ,  。 / x ,  "
     ]
    }
   ],
   "source": [
    "for w in result:\n",
    "    print(w.word, \"/\", w.flag, \", \", end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_install/ /is/ /great\n"
     ]
    }
   ],
   "source": [
    "terms = jieba.cut('easy_install is great')\n",
    "print('/'.join(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python/ /的/正则表达式/是/好用/的\n"
     ]
    }
   ],
   "source": [
    "terms = jieba.cut('python 的正则表达式是好用的')\n",
    "print('/'.join(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlist = [\n",
    "    ('今天天气不错', ('今天', '天气')),\n",
    "    ('如果放到post中将出错。', ('中', '将')),\n",
    "    ('我们中出了一个叛徒', ('中', '出')),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气/不错\n",
      "今天天气 Before: 3, After: 0\n",
      "今天/天气/不错\n",
      "----------------------------------------\n",
      "如果/放到/post/中将/出错/。\n",
      "中将 Before: 763, After: 494\n",
      "如果/放到/post/中/将/出错/。\n",
      "----------------------------------------\n",
      "我们/中/出/了/一个/叛徒\n",
      "中出 Before: 3, After: 3\n",
      "我们/中/出/了/一个/叛徒\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent, seg in testlist:\n",
    "    print('/'.join(jieba.cut(sent, HMM=False)))\n",
    "    word = ''.join(seg)\n",
    "    print('%s Before: %s, After: %s' % (word, jieba.get_FREQ(word), jieba.suggest_freq(seg, True)))\n",
    "    print('/'.join(jieba.cut(sent, HMM=False)))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词频省略时使用自动计算的能保证分出该词的词频。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更改分词器（默认为 jieba.dt）的 tmp_dir 和 cache_file 属性，可分别指定缓存文件所在的文件夹及其文件名，用于受限的文件系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调整词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 add_word(word, freq=None, tag=None) 和 del_word(word) 可在程序中动态修改词典。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：自动计算的词频在使用 HMM 新词发现功能时可能无效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/post/中/将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq(('中', '将'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如果/放到/post/中/将/出错/。\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('如果放到post中将出错。', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「/台中/」/正确/应该/不会/被/切开\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq('台中', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「/台中/」/正确/应该/不会/被/切开\n"
     ]
    }
   ],
   "source": [
    "print('/'.join(jieba.cut('「台中」正确应该不会被切开', HMM=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"通过用户自定义词典来增强歧义纠错能力\" --- https://github.com/fxsjy/jieba/issues/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于 TF-IDF 算法的关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jieba.analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### · jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence 为待提取的文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withWeight 为是否一并返回关键词权重值，默认值为 False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allowPOS 仅包括指定词性的词，默认值为空，即不筛选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "· jieba.analyse.TFIDF(idf_path=None) 新建 TFIDF 实例，idf_path 为 IDF 频率文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码示例 （关键词提取）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage:    python extract_tags.py [file name] -k [top k]\n",
      "\n",
      "ipykernel_launcher.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from optparse import OptionParser\n",
    "\n",
    "USAGE = \"usage:    python extract_tags.py [file name] -k [top k]\"\n",
    "\n",
    "parser = OptionParser(USAGE)\n",
    "parser.add_option(\"-k\", dest=\"topK\")\n",
    "opt, args = parser.parse_args()\n",
    "\n",
    "\n",
    "if len(args) < 1:\n",
    "    print(USAGE)\n",
    "    sys.exit(1)\n",
    "\n",
    "file_name = args[0]\n",
    "\n",
    "if opt.topK is None:\n",
    "    topK = 10\n",
    "else:\n",
    "    topK = int(opt.topK)\n",
    "\n",
    "content = open(file_name, 'rb').read()\n",
    "\n",
    "tags = jieba.analyse.extract_tags(content, topK=topK)\n",
    "\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键词提取所使用逆向文件频率（IDF）文本语料库可以切换成自定义语料库的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义语料库示例：https://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用法示例：https://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage:    python extract_tags_idfpath.py [file name] -k [top k]\n",
      "\n",
      "ipykernel_launcher.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from optparse import OptionParser\n",
    "\n",
    "USAGE = \"usage:    python extract_tags_idfpath.py [file name] -k [top k]\"\n",
    "\n",
    "parser = OptionParser(USAGE)\n",
    "parser.add_option(\"-k\", dest=\"topK\")\n",
    "opt, args = parser.parse_args()\n",
    "\n",
    "\n",
    "if len(args) < 1:\n",
    "    print(USAGE)\n",
    "    sys.exit(1)\n",
    "\n",
    "file_name = args[0]\n",
    "\n",
    "if opt.topK is None:\n",
    "    topK = 10\n",
    "else:\n",
    "    topK = int(opt.topK)\n",
    "\n",
    "content = open(file_name, 'rb').read()\n",
    "\n",
    "jieba.analyse.set_idf_path(\"./dict/idf.txt\");\n",
    "\n",
    "tags = jieba.analyse.extract_tags(content, topK=topK)\n",
    "\n",
    "print(\",\".join(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义语料库示例：https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用法示例：https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键词一并返回关键词权重值示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用法示例：https://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于 TextRank 算法的关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.analyse.TextRank() 新建自定义 TextRank 实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法论文： TextRank: Bringing Order into Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基本思想:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将待抽取关键词的文本进行分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用示例：\n",
    "https://github.com/fxsjy/jieba/blob/master/test/demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.posseg.POSTokenizer(tokenizer=None) 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用法示例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "爱 v\n",
      "北京 ns\n",
      "天安门 ns\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我爱北京天安门\")\n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并行分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，然后归并结果，从而获得分词速度的可观提升。基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows。用法如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.disable_parallel() # 关闭并行分词模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子：https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验结果：在 4 核 3.4GHz Linux 机器上，对金庸全集进行精确分词，获得了 1MB/s 的速度，是单进程版的 3.3 倍。\n",
    "\n",
    "注意：并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize：返回词语在原文的起止位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，输入参数只接受 unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 默认模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'永和服装饰品有限公司')\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 搜索模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 永和\t\t start: 0 \t\t end:2\n",
      "word 服装\t\t start: 2 \t\t end:4\n",
      "word 饰品\t\t start: 4 \t\t end:6\n",
      "word 有限\t\t start: 6 \t\t end:8\n",
      "word 公司\t\t start: 8 \t\t end:10\n",
      "word 有限公司\t\t start: 6 \t\t end:10\n"
     ]
    }
   ],
   "source": [
    "result = jieba.tokenize(u'永和服装饰品有限公司', mode='search')\n",
    "for tk in result:\n",
    "    print(\"word %s\\t\\t start: %d \\t\\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChineseAnalyzer for Whoosh 搜索引擎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引用： from jieba.analyse import ChineseAnalyzer\n",
    "用法示例：https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 命令行分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用示例：python -m jieba news.txt > cut_result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "命令行选项（翻译）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "使用: python -m jieba [options] filename\n",
    "\n",
    "结巴命令行界面。\n",
    "\n",
    "固定参数:\n",
    "  filename              输入文件\n",
    "\n",
    "可选参数:\n",
    "  -h, --help            显示此帮助信息并退出\n",
    "  -d [DELIM], --delimiter [DELIM]\n",
    "                        使用 DELIM 分隔词语，而不是用默认的' / '。\n",
    "                        若不指定 DELIM，则使用一个空格分隔。\n",
    "  -p [DELIM], --pos [DELIM]\n",
    "                        启用词性标注；如果指定 DELIM，词语和词性之间\n",
    "                        用它分隔，否则用 _ 分隔\n",
    "  -D DICT, --dict DICT  使用 DICT 代替默认词典\n",
    "  -u USER_DICT, --user-dict USER_DICT\n",
    "                        使用 USER_DICT 作为附加词典，与默认词典或自定义词典配合使用\n",
    "  -a, --cut-all         全模式分词（不支持词性标注）\n",
    "  -n, --no-hmm          不使用隐含马尔可夫模型\n",
    "  -q, --quiet           不输出载入信息到 STDERR\n",
    "  -V, --version         显示版本信息并退出\n",
    "\n",
    "如果没有指定文件名，则使用标准输入。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--help 选项输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$> python -m jieba --help\n",
    "Jieba command line interface.\n",
    "\n",
    "positional arguments:\n",
    "  filename              input file\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -d [DELIM], --delimiter [DELIM]\n",
    "                        use DELIM instead of ' / ' for word delimiter; or a\n",
    "                        space if it is used without DELIM\n",
    "  -p [DELIM], --pos [DELIM]\n",
    "                        enable POS tagging; if DELIM is specified, use DELIM\n",
    "                        instead of '_' for POS delimiter\n",
    "  -D DICT, --dict DICT  use DICT as dictionary\n",
    "  -u USER_DICT, --user-dict USER_DICT\n",
    "                        use USER_DICT together with the default dictionary or\n",
    "                        DICT (if specified)\n",
    "  -a, --cut-all         full pattern cutting (ignored with POS tagging)\n",
    "  -n, --no-hmm          don't use the Hidden Markov Model\n",
    "  -q, --quiet           don't print loading messages to stderr\n",
    "  -V, --version         show program's version number and exit\n",
    "\n",
    "If no filename specified, use STDIN instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 延迟加载机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba 采用延迟加载，import jieba 和 jieba.Tokenizer() 不会立即触发词典的加载，一旦有必要才开始加载词典构建前缀字典。如果你想手工初始 jieba，也可以手动初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.initialize()  # 手动初始化（可选）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 0.28 之前的版本是不能指定主词典的路径的，有了延迟加载机制后，你可以改变主词典的路径:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba.set_dictionary('data/dict.txt.big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子： https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他词典\n",
    "占用内存较小的词典文件 https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small\n",
    "\n",
    "支持繁体分词更好的词典文件 https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\n",
    "\n",
    "下载你所需要的词典，然后覆盖 jieba/dict.txt 即可；或者用 jieba.set_dictionary('data/dict.txt.big')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## github参考\n",
    "https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 友情链接\n",
    "https://github.com/baidu/lac 百度中文词法分析（分词+词性+专名）系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/baidu/AnyQ 百度FAQ自动问答系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/baidu/Senta 百度情感识别系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以用NLTK做一些简单的事情"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标记和标记一些文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged[0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 确定被点名的实体："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
